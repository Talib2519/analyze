# Data Processing Project

This project demonstrates a robust data processing workflow using Python with Pandas, integrated with GitHub Actions for continuous integration and deployment. It processes an Excel file (`data.xlsx`), converts it to CSV, and then performs a statistical analysis, publishing the results as a JSON file via GitHub Pages.

## Project Structure

*   `execute.py`: The core Python script responsible for data processing.
*   `data.xlsx`: The original Excel data file.
*   `data.csv`: The converted CSV version of `data.xlsx`, committed to the repository for `execute.py` to read.
*   `.github/workflows/ci.yml`: GitHub Actions workflow for linting, testing, and deployment.
*   `index.html`: A simple landing page for the project, linking to the generated `result.json`.
*   `LICENSE`: The MIT License for the project.

## Features

*   **Data Ingestion**: Reads data from `data.csv` (originally `data.xlsx`).
*   **Robust Data Processing**: The `execute.py` script includes a fix for a non-trivial error, ensuring data integrity and correct statistical aggregation even with mixed data types or missing values. Specifically, it now correctly handles potential non-numeric entries in the 'Amount' column by coercing them to numeric types and filling NaNs.
*   **Monthly Aggregation**: Groups data by month and calculates the total amount for each month.
*   **JSON Output**: Publishes the processed data as a JSON file (`result.json`).
*   **Continuous Integration (CI)**:
    *   **Code Linting**: Uses `ruff` to maintain code quality and style.
    *   **Automated Execution**: Runs `execute.py` on every push to generate `result.json`.
*   **Continuous Deployment (CD)**:
    *   **GitHub Pages Publication**: `result.json` is automatically published to GitHub Pages, making the analysis results publicly accessible.

## Setup

### Prerequisites

*   Python 3.11+
*   Pandas 2.3+
*   `openpyxl` (required for converting `data.xlsx` to `data.csv`)
*   `ruff` for linting

### Installation

1.  Clone the repository:
    ```bash
    git clone https://github.com/your-username/your-repo-name.git
    cd your-repo-name
    ```
2.  Create and activate a virtual environment (recommended):
    ```bash
    python -m venv venv
    source venv/bin/activate # On Windows: .\venv\Scripts\activate
    ```
3.  Install dependencies:
    ```bash
    pip install pandas openpyxl ruff
    ```

## Usage

### Data Conversion

The `data.xlsx` file should be converted to `data.csv` and committed to the repository. If you have `data.xlsx`, you can convert it locally using:

```bash
python -c "import pandas as pd; pd.read_excel('data.xlsx').to_csv('data.csv', index=False)"
```

### Running the Script Locally

To run the data processing script and generate `result.json` locally (ensure `data.csv` is present):

```bash
python execute.py > result.json
```

This will create a `result.json` file in your current directory containing the monthly aggregated data.

### Linting

To run `ruff` for code linting:

```bash
ruff check .
```

### GitHub Actions Workflow

The `.github/workflows/ci.yml` workflow automatically performs the following steps on every `push` event to any branch:

1.  **Setup Python**: Configures the environment with Python 3.11.
2.  **Install Dependencies**: Installs `pandas`, `openpyxl`, and `ruff`.
3.  **Create Mock `data.csv`**: For CI example, a dummy `data.csv` is created. In a real repository, this file would be committed.
4.  **Run Ruff**: Checks code style and potential errors using `ruff`.
5.  **Execute Script**: Runs `python execute.py` and redirects its output to `_site/result.json`.
6.  **Upload Artifact**: Uploads `_site` as a GitHub Pages artifact.
7.  **Deploy to GitHub Pages**: Publishes the artifact to GitHub Pages.

## Viewing Results

The `result.json` generated by the CI pipeline is published to GitHub Pages. You can access it directly via a URL like:

`https://your-username.github.io/your-repo-name/result.json`

(Replace `your-username` and `your-repo-name` with your actual GitHub details).

The `index.html` file in this repository also provides a convenient link to this `result.json` once deployed.

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## Appendix: Files for direct use in repository

### `execute.py` content
```python
import pandas as pd
import sys
import json

def process_data(file_path):
    try:
        df = pd.read_csv(file_path)
    except FileNotFoundError:
        print(json.dumps({"error": f"File not found: {file_path}"}), file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(json.dumps({"error": f"Error reading CSV: {e}"}), file=sys.stderr)
        sys.exit(1)

    # Nontrivial error fix: Ensure 'Amount' column is numeric and handle missing values.
    # The original script might have failed if 'Amount' contained non-numeric strings
    # or produced incorrect sums if NaNs were present and not handled.
    # Fix: Convert 'Amount' to numeric, coercing errors to NaN, then fill NaNs with 0
    # before performing the sum, or drop rows with NaNs if that's the desired behavior.
    # For a sum, filling with 0 is often appropriate.
    df['Amount'] = pd.to_numeric(df['Amount'], errors='coerce')
    df['Amount'] = df['Amount'].fillna(0) # Fill NaNs with 0 for summation

    try:
        df['Date'] = pd.to_datetime(df['Date'])
    except Exception as e:
        print(json.dumps({"error": f"Error converting 'Date' column: {e}"}), file=sys.stderr)
        sys.exit(1)

    df['Month'] = df['Date'].dt.to_period('M')

    # Group by month and sum the now cleaned 'Amount'
    monthly_summary = df.groupby('Month')['Amount'].sum().reset_index()
    monthly_summary['Month'] = monthly_summary['Month'].astype(str) # Convert Period to string for JSON

    print(monthly_summary.to_json(orient='records', indent=2))

if __name__ == "__main__":
    process_data("data.csv")

```

### `.github/workflows/ci.yml` content
```yaml
name: CI/CD Pipeline

on:
  push:
    branches:
      - main # Or master, depending on your default branch

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest

    permissions:
      contents: read
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas openpyxl ruff

      - name: Create mock data.csv for CI run
        # In a real repository, data.csv would be committed, having been converted from data.xlsx.
        # This step creates a dummy data.csv to ensure the workflow is runnable as an example.
        run: |
          echo "Date,Amount,Category" > data.csv
          echo "2023-01-05,100,A" >> data.csv
          echo "2023-01-15,150,B" >> data.csv
          echo "2023-02-10,200,A" >> data.csv
          echo "2023-02-20,abc,C" >> data.csv # This will trigger the error fix in execute.py
          echo "2023-03-01,300,B" >> data.csv
          echo "2023-03-10,,A" >> data.csv # This will trigger the NaN fill in execute.py

      - name: Run Ruff
        run: ruff check .

      - name: Execute script and generate result.json
        run: |
          mkdir -p _site # Create _site directory for GitHub Pages
          python execute.py > _site/result.json

      - name: Upload GitHub Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: '_site' # The directory where result.json is located

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
```